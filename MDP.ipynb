{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we introduce reinforcement learning formalised as a Markov Decision Process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Decision Processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**State**: $s_t \\in \\mathbb{S}$\n",
    "\n",
    "**Action**: $a_t \\in \\mathbb{A}$\n",
    "\n",
    "**Reward**: $r_t \\in \\mathbb{R}$\n",
    "\n",
    "**Policy**: $\\Pi_k(a|s) = \\text{P}(a_k = a|s_k = s)$\n",
    "\n",
    "**Model**: $P(s', r|s, a)$\n",
    "\n",
    "**Objective**: $\\max_{\\mathbf{a}}\\sum_{k=0}^\\infty \\lambda^k r(s,a)$\n",
    "\n",
    "**Markove Property**: $\\text{Pr}(s_{k+1} = s', r_k = r|s_k, a_k, r_{k-1}, s_{k-1}, a_{k-1},...) = \\text{Pr}(s_{k+1} = s', r_k = r|s_k, a_k)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the **objective** can be rewritten as \n",
    "\n",
    "$$\\min_{\\mathbf{u}}\\sum_{k=0}^\\infty \\lambda^k c_k(x_k, u_k)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Referring back to [`optimal_control.ipynb`](optimal_control.ipynb) we can easily show the new Gellman equation is\n",
    "\n",
    "$$V(x, k) = \\min_u(c(x, u) + \\lambda V(f(x, u), k + 1)), \\quad k = h-1, h-2, ..., 1, 0$$\n",
    "\n",
    "where $\\lambda \\leq 1$ is the discount factor, lower indicates we prioritise ealier rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Episodic problems are finite horizon which stop when $\\forall x \\in X_s$ (stopping set X_s):\n",
    "\n",
    "- $f(x,u) \\in X_s$\n",
    "- c(x, u) = 0\n",
    "\n",
    "As a defined stopping set is given, we don't require separate values for the same state at different times, hence yielding\n",
    "\n",
    "$$V(x) = \\min_u(c(x, u) + \\lambda V(f(x, u)))$$\n",
    "\n",
    "solved by\n",
    "\n",
    "$$V_{k+1}(x) = \\min_u(c(x, u) + \\lambda V_k(f(x, u)))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Proof of Convergence** $(\\lambda < 1)$:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "|BV_1(x) - BV_2(x)| &= |\\min_u(c(x, u) + \\lambda V_1(f(x, u))) - \\min_u(c(x, u) + \\lambda V_2(f(x, u)))| \\\\\n",
    "&\\leq \\max_u|\\lambda V_1(f(x,u))- \\lambda V_2(f(x,u))| \\\\\n",
    "&= \\lambda \\max_u| V_1(f(x,u))-  V_2(f(x,u))|\n",
    "\n",
    "\\end{align*}\n",
    "$$\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
