{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimal Control\n",
    "\n",
    "Understanding the fundamentals of optimal control is important before moving onto reinforcement learning. Classical control has been researched thoroughly with the robustness and behaviour well understood. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discrete-time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamics\n",
    "\n",
    "$$x_{k+1} = f(x_k, u_k), \\text{ where } f(x, u) = Ax + Bu \\text{ for example} $$\n",
    "\n",
    "### Trajectory\n",
    "\n",
    "Given $x_0$, each input sequence, $u_0$, $u_1$, ... ,$u_{h-1}$, generates a state sequence $x_0$, $x_1$, ... ,$x_h$.\n",
    "\n",
    "### Cost Function\n",
    "\n",
    "$$J(x_0, u_0, u_1, ... ,u_{h-1}) = \\sum^{h-1}_{k=0} c(x_k, u_k) + J_h(x_h)$$\n",
    "\n",
    "where $c(x_k, u_k)$ is the stage cost and $J_h(x_h)$ is the terminal cost.\n",
    "\n",
    "### Objective\n",
    "\n",
    "$$\\begin{align*}\n",
    "J^*(x_0) &= J(x_0, u_0^*, u_1^*, ..., u_{n-1}^*) \\\\\n",
    "&= \\min_{u_0,u_1,...,u_{n-1}} J(x_0, u_0, ..., u_{n-1})\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bellman's Principle of Optimality\n",
    "\n",
    "$$V(x, k) \\triangleq \\min_{u_k,u_{k+1},...,u_{h-1}} \\left(\\sum_{i=k}^{h-1} c(x_i, u_i) + J_h(x_h)\\right)$$\n",
    "\n",
    "$V(x, k)$ is the value function or cost-to-go\n",
    "\n",
    "$$\\begin{align*}\n",
    "V(x, k) &= \\min_{u_k,u_{k+1},...,u_{h-1}} \\left(\\sum_{i=k}^{h-1} c(x_i, u_i) + J_h(x_h)\\right) \\\\\n",
    "&= \\min_{u_k,u_{k+1},...,u_{h-1}} \\left(c(x_k, u_k) + \\sum_{i=k+1}^{h-1} c(x_i, u_i) + J_h(x_h)\\right) \\\\\n",
    "&= \\min_{u_k} \\left(\\min_{u_{k+1},...,u_{h-1}} \\left(c(x_k, u_k) + \\sum_{i=k+1}^{h-1} c(x_i, u_i) + J_h(x_h)\\right)\\right) \\\\\n",
    "&= \\min_{u_k} \\left(c(x_k, u_k) + \\min_{u_{k+1},...,u_{h-1}} \\left(\\sum_{i=k+1}^{h-1} c(x_i, u_i) + J_h(x_h)\\right)\\right) \\\\\n",
    "&= \\min_{u_k}(c(x_k, u_k) + V(x_{k+1}, k + 1))\n",
    "\\end{align*}$$\n",
    "\n",
    "where $V(x, h) = J_h(x)$ and $ V(x_0, 0) = J^*(x_0)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal Input\n",
    "\n",
    "$$g(x, k) = \\arg\\min_u(c(x, u) + V(f(x, u), k + 1))$$\n",
    "\n",
    "Then the optimal control is given by\n",
    "\n",
    "$$u_k^* = g(x_k, k), \\quad k = 0, 1, ..., h-1.$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Linear Quadratic Regulator \n",
    "\n",
    "$$x_{k+1} = Ax_k + Bu_k $$\n",
    "\n",
    "$$J(x_0, u_0, u_1, ... ,u_{h-1}) = \\sum^{h-1}_{k=0} \\left( x^T_kQx_k + u^T_kRu_k \\right) + x^T_hX_hx_h$$\n",
    "\n",
    "$Q$, $R$, $X_h$ are symmetric matrices with $Q \\geq 0$, $R > 0$ and $X_h \\geq 0$.\n",
    "\n",
    "$$V(x, k) = \\min_{u} \\left(  x^TQx + u^TRu + V(Ax+Bu, k+1)\\right) $$\n",
    "\n",
    "so \n",
    "\n",
    "$$\\begin{align*}\n",
    "V(x, h-1) &= \\min_{u} \\left(  x^TQx + u^TRu + (Ax+Bu)^TX_h(Ax+Bu)\\right) \\\\\n",
    "&= \\min_{u} \\begin{bmatrix}x^T & u^T\\end{bmatrix} \\begin{bmatrix}Q + A^T X_h A & A^T X_h B \\\\ B^T X_h A & R + B^T X_h B\\end{bmatrix} \\begin{bmatrix}x \\\\ u\\end{bmatrix}\n",
    "\\end{align*}$$\n",
    "\n",
    "Lemma 1: Minimisation of quadratic forms\n",
    "\n",
    "$$\\min_u \\begin{bmatrix}x^T & u^T\\end{bmatrix} \\begin{bmatrix}Q & S^T \\\\ S & R\\end{bmatrix} \\begin{bmatrix}x \\\\ u\\end{bmatrix} = x^T(Q - S^TR^{-1}S)x$$\n",
    "\n",
    "and the minimum is achieved at\n",
    "\n",
    "$$u = -R^{-1}Sx.$$\n",
    "\n",
    "This is simply shown by multiplying out the matrices, taking the derivative w.r.t. **u** and finding the stationary point.\n",
    "\n",
    "Therefore the solution to the Dynamic programming equation is given by\n",
    "\n",
    "$$ u_{h-1} = -(R + B^TX_{h}B)^{-1}B^TX_{h}Ax_{h-1}$$\n",
    "\n",
    "so\n",
    "\n",
    "$$\\begin{align*}\n",
    "V(x, h-1) &= x^T (Q + A^TX_hA - A^TX_hB(R + B^TX_{h}B)^{-1}B^TX_{h}A) x \\\\\n",
    "          &= x^T X_{h-1} x\n",
    "\\end{align*}$$\n",
    "\n",
    "Finally\n",
    "\n",
    "$$\\begin{align*}\n",
    "V(x, h) &= x^T X_{h} x \\\\\n",
    "V(x, h-1)  &= x^T X_{h-1} x \\\\\n",
    "       & ... \\\\\n",
    "V(x, 0)  &= x^T X_{0} x \\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "The optimal cost is now given by $x^T X_{0} x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemma 2:  $X_k >= 0$ for $k=h,h-1,...,0$ if $X_h >= 0$, $Q >= 0$ and $R > 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Infinite Horizon\n",
    "\n",
    "In the above section we explored the Discrete-time Finite Horizon case, but in the case of an infinite hoirzon behaviour differs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cost function now lacks the X matrix, but this is not fully removed from our solution\n",
    "\n",
    "$$J(x_0, u_0, u_1, ... ,u_{h-1}) = \\sum^{h-1}_{k=0} \\left( x^T_kQx_k + u^T_kRu_k \\right)$$\n",
    "\n",
    "We proposes a solution similar to the one above, but now with a singular X matrix. The idea is that the 'terminal cost' for the terms further in the horizon contain an infinite number of terms following the recursion of the dynamic eq. above; hence we represent it as a single matrix as it asumes stationarity with no time influence. \n",
    "\n",
    "$$ u_{k} = -(R + B^TXB)^{-1}B^TXAx_{k}$$\n",
    "\n",
    "Where $X = X^T > 0$ solves the **Discrete Algebraic Riccati Equation**\n",
    "\n",
    "$$ X = Q + A^TXA - A^TXB(R + B^TXB)^{-1}B^TXA $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous-time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamics\n",
    "\n",
    "$$\\dot{x} = f(x, u), \\text{ e.g. } \\dot{x} = \\sin{x} + u $$\n",
    "\n",
    "### Trajectory\n",
    "\n",
    "Given $x_0 \\in X$ and a horizon $T \\geq 0$, each input function $u(\\cdot) : [0,T] \\to U$ generates a state trajectory\n",
    "$x(\\cdot) : [0,T] \\to \\mathbb{R}^n$ such that $x(0) = x_0$ and $\\forall t \\in [0,T] : x(t) = f(x(t),u(t))$.\n",
    "\n",
    "### Cost Function\n",
    "\n",
    "$$J(x_0, u(\\cdot)) = \\int^T_0 c(x(t), u(t)) + J_T(x_T)$$\n",
    "\n",
    "where $c(x_k, u_k)$ is the stage cost and $J_h(x_h)$ is the terminal cost.\n",
    "\n",
    "### Objective\n",
    "\n",
    "$$\\begin{align*}\n",
    "J^*(x_0) &= J(x_0, u^*(\\cdot)) \\\\\n",
    "&= \\min_{u(\\cdot)} J(x_0, u(\\cdot))\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bellman's Principality of Optimality\n",
    "\n",
    "$$V(x(t), t) \\triangleq \\min_{u(\\cdot)} \\left(\\int_t^T c(x(\\tau), u(\\tau)) d\\tau + J_T(x(T))\\right)$$\n",
    "\n",
    "following the discrete case, we are able to obtain recursion:\n",
    "\n",
    "$$\\begin{align*}\n",
    "V(x(t), t) &= \\min_{u(\\cdot)} \\left(\\int_t^{t+h} c(x(\\tau), u(\\tau)) d\\tau + \\int_{t+h}^T c(x(\\tau), u(\\tau)) d\\tau + J_T(x(T))\\right) \\\\\n",
    "&= \\min_{u(\\cdot)} \\left(\\int_t^{t+h} c(x(\\tau), u(\\tau)) d\\tau + V(x(t+h), t+h)\\right)\n",
    "\\end{align*}$$\n",
    "\n",
    "where $V(x(T), T) = J_T(x(T))$ and $V(x(0), 0) = J^*(x(0))$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal Input\n",
    "\n",
    "first we approximate the continous dynamics\n",
    "\n",
    "$$x(t+h) = x(t) + f(x(t),u(t))h + \\mathcal{O}(h^2)$$\n",
    "\n",
    "and the incremental cost accumalation\n",
    "\n",
    "$$\\int_t^{t+h} c(x(\\tau),u(\\tau)) d\\tau = c(x(t),u(t))h + \\mathcal{O}(h^2)$$\n",
    "\n",
    "subbing into the Bellman's gives\n",
    "\n",
    "$$V(x,t) = \\min_{u \\in U} \\left(c(x,u)h + V(x + f(x,u)h, t + h)\\right) + \\mathcal{O}(h^2)$$\n",
    "\n",
    "recall that\n",
    "\n",
    "$$\\frac{\\partial V}{\\partial x} \\triangleq \\left[\\frac{\\partial V}{\\partial x_1} \\quad \\frac{\\partial V}{\\partial x_2} \\quad \\frac{\\partial V}{\\partial x_3} \\quad \\cdots \\quad \\frac{\\partial V}{\\partial x_n}\\right]$$\n",
    "\n",
    "therefore \n",
    "\n",
    "$$V(x,t) = \\min_{u \\in U} \\left(c(x,u)h + V(x, t) + \\frac{\\partial V(x, t)}{\\partial x} f(x, u) h + \\frac{\\partial V(x, t)}{\\partial t} h \\right) + \\mathcal{O}(h^2)$$\n",
    "\n",
    "rearrange to finally give **Hamilton-Jacobi-Bellman PDE**\n",
    "\n",
    "$$ - \\frac{\\partial V(x, t)}{\\partial t} = \\min_{u \\in U} \\left(c(x,u) + \\frac{\\partial V(x, t)}{\\partial x} f(x, u)  \\right) $$\n",
    "\n",
    "with the boundary condition of $V(x(T), T) = J_T(x(T))$ the optimal input is \n",
    "\n",
    "$$ u^*(t) =  \\min_{u \\in U} \\left(c(x,u) + \\frac{\\partial V(x, t)}{\\partial x} f(x, u)  \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Linear Quadratic Regulator "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\dot{x} = Ax + Bu $$\n",
    "\n",
    "$$ c(x, u) = x^T Q x + u^T R u $$\n",
    "\n",
    "$$ J_T(x) = x^T X_T x $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subsituting the above into the Hamilton-jacobi-Bellman equation, we get the following equation\n",
    "\n",
    "\n",
    "$$ x^T \\dot{X(t)} x = \\min_u \\left( x^T Q x + u^T R u + 2x^TX(t) (Ax + Bu) \\right) $$\n",
    "\n",
    "Using Lemma 1 we get an optimal input\n",
    "\n",
    "$$ u^*(t) =  -R^{-1}B^TX(t)x(t)$$\n",
    "\n",
    "Which gives\n",
    "\n",
    "$$ x^T \\dot{X(t)} x = x^T \\left( Q + XA +A^TX - XBR^{-1}B^TX  \\right) x$$\n",
    "\n",
    "This is then solved via backward numerical integration\n",
    "\n",
    "$$ \\dot{X(t)} \\approx \\frac{X(t) - X(t - \\Delta t)}{\\Delta t} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Tuple\n",
    "\n",
    "\n",
    "def lqr_backwards_sweep(\n",
    "    A: np.ndarray,\n",
    "    B: np.ndarray,\n",
    "    Q: np.ndarray,\n",
    "    R: np.ndarray,\n",
    "    XN: np.ndarray,\n",
    "    dt: float,\n",
    "    trange: np.ndarray,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute backwards integration of the Riccati equation.\n",
    "    \"\"\"\n",
    "    iR = np.linalg.inv(R)\n",
    "    trange_reverse = np.flip(trange)\n",
    "    n = A.shape[0]\n",
    "    Xlog = np.zeros((len(trange), n, n))\n",
    "\n",
    "    idx = len(trange_reverse) - 1\n",
    "    Xlog[idx] = XN\n",
    "    X = XN.copy()\n",
    "\n",
    "    for _ in trange_reverse[:-1]:\n",
    "        idx -= 1\n",
    "        Xdot = -(Q + X @ A + A.T @ X - X @ B @ iR @ B.T @ X)\n",
    "        X = X - Xdot * dt\n",
    "        Xlog[idx] = X\n",
    "\n",
    "    return Xlog\n",
    "\n",
    "\n",
    "def lqr_forwards_sim(\n",
    "    A: np.ndarray,\n",
    "    B: np.ndarray,\n",
    "    R: np.ndarray,\n",
    "    x0: np.ndarray,\n",
    "    Xlog: np.ndarray,\n",
    "    dt: float,\n",
    "    trange: np.ndarray,\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Run the system forwards with the computed optimal control.\n",
    "    \"\"\"\n",
    "    iR = np.linalg.inv(R)\n",
    "    n = x0.shape[0]\n",
    "    xlog = np.zeros((len(trange), n))\n",
    "    ulog = np.zeros(len(trange) - 1)\n",
    "\n",
    "    idx = 0\n",
    "    x = x0.reshape(-1)  # Convert to 1D array while preserving values\n",
    "    xlog[idx] = x\n",
    "\n",
    "    for _ in trange[:-1]:\n",
    "        X = Xlog[idx]\n",
    "        u_mat = -iR @ B.T @ X @ x.reshape(-1, 1)\n",
    "        u = u_mat[0, 0]  # Extract scalar value properly\n",
    "        ulog[idx] = u\n",
    "        idx += 1\n",
    "        dx = (A @ x.reshape(-1, 1) + B * u).reshape(-1) * dt\n",
    "        x = x + dx\n",
    "        xlog[idx] = x\n",
    "\n",
    "    return xlog, ulog\n",
    "\n",
    "\n",
    "def run_lqr_example():\n",
    "    # State space model\n",
    "    A = np.array([[0, 1], [4, 0]])\n",
    "    B = np.array([[1], [-1]])\n",
    "    C = np.array([[1, 0]])\n",
    "\n",
    "    # Cost function\n",
    "    Q = C.T @ C\n",
    "    R = np.array([[1]])\n",
    "\n",
    "    # Terminal costs (two examples)\n",
    "    XN_1 = 20 * np.eye(2)\n",
    "    XN_2 = np.zeros((2, 2))\n",
    "\n",
    "    # Discretisation parameters\n",
    "    dt = 0.001\n",
    "    T = 4\n",
    "    trange = np.arange(0, T + dt, dt)\n",
    "\n",
    "    # Backwards sweep\n",
    "    Xlog_1 = lqr_backwards_sweep(A, B, Q, R, XN_1, dt, trange)\n",
    "    Xlog_2 = lqr_backwards_sweep(A, B, Q, R, XN_2, dt, trange)\n",
    "\n",
    "    # Forward simulation\n",
    "    x0 = np.array([[1], [1]])\n",
    "    xlog_1, ulog_1 = lqr_forwards_sim(A, B, R, x0, Xlog_1, dt, trange)\n",
    "    xlog_2, ulog_2 = lqr_forwards_sim(A, B, R, x0, Xlog_2, dt, trange)\n",
    "\n",
    "    # Compute optimal costs properly extracting scalar values\n",
    "    cost_1 = x0.T @ Xlog_1[0] @ x0\n",
    "    cost_2 = x0.T @ Xlog_2[0] @ x0\n",
    "    optimal_cost_1 = cost_1[0, 0]\n",
    "    optimal_cost_2 = cost_2[0, 0]\n",
    "\n",
    "    return (\n",
    "        trange,\n",
    "        Xlog_1,\n",
    "        Xlog_2,\n",
    "        xlog_1,\n",
    "        xlog_2,\n",
    "        ulog_1,\n",
    "        ulog_2,\n",
    "        optimal_cost_1,\n",
    "        optimal_cost_2,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = run_lqr_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract the time range and xlog values from the results\n",
    "trange, _, _, xlog_1, xlog_2, _, _, _, _ = results\n",
    "\n",
    "# Plot xlog_1\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(trange, xlog_1[:, 0], label=\"x_1 (XN_1)\", linestyle=\"-\")\n",
    "plt.plot(trange, xlog_1[:, 1], label=\"x_2 (XN_1)\", linestyle=\"--\")\n",
    "\n",
    "# Plot xlog_2\n",
    "plt.plot(trange, xlog_2[:, 0], label=\"x_1 (XN_2)\", linestyle=\"-\")\n",
    "plt.plot(trange, xlog_2[:, 1], label=\"x_2 (XN_2)\", linestyle=\"--\")\n",
    "\n",
    "# Add labels and legend\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"State\")\n",
    "plt.title(\"State Trajectories\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Infinite Horizon LQR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cost function is now\n",
    "\n",
    "$$J(x_0, u(\\cdot)) = \\int_0^\\infty c(x(t), u(t))$$\n",
    "\n",
    "Following on from the discrete case, we observer that the time dependency of X must now not exist. This yields\n",
    "\n",
    "$$ u^*(t) =  -R^{-1}B^TXx(t)$$\n",
    "\n",
    "Which gives the **Control Algebraic Riccati Equation** solved by $X = X^T > 0$\n",
    "\n",
    "$$ 0 =  Q + XA +A^TX - XBR^{-1}B^TX  $$\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
