{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we introduce reinforcement learning formalised as a Markov Decision Process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Decision Processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**State**: $s_t \\in \\mathbb{S}$\n",
    "\n",
    "**Action**: $a_t \\in \\mathbb{A}$\n",
    "\n",
    "**Reward**: $r_t \\in \\mathbb{R}$\n",
    "\n",
    "**Policy**: $\\Pi_k(a|s) = p(a_k = a|s_k = s)$\n",
    "\n",
    "**Model**: $p(s', r|s, a)$\n",
    "\n",
    "**Objective**: $\\max_{\\mathbf{a}}\\sum_{k=0}^\\infty \\lambda^k r(s,a)$\n",
    "\n",
    "**Markove Property**: $p(s_{k+1} = s', r_k = r|s_k, a_k, r_{k-1}, s_{k-1}, a_{k-1},...) = p(s_{k+1} = s', r_k = r|s_k, a_k)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the **objective** can be rewritten as \n",
    "\n",
    "$$\\min_{\\mathbf{u}}\\sum_{k=0}^\\infty \\lambda^k c_k(x_k, u_k)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Referring back to [`1_optimal_control.ipynb`](1_optimal_control.ipynb) we can easily show the new Gellman equation is\n",
    "\n",
    "$$V(x, k) = \\min_u(c(x, u) + \\lambda V(f(x, u), k + 1)), \\quad k = h-1, h-2, ..., 1, 0$$\n",
    "\n",
    "where $\\lambda \\leq 1$ is the discount factor, lower indicates we prioritise ealier rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Episodic problems are finite horizon which stop when $\\forall x \\in X_s$ (stopping set X_s):\n",
    "\n",
    "- $f(x,u) \\in X_s$\n",
    "- c(x, u) = 0\n",
    "\n",
    "As a defined stopping set is given, we don't require separate values for the same state at different times, hence yielding\n",
    "\n",
    "$$V(x) = \\min_u(c(x, u) + \\lambda V(f(x, u)))$$\n",
    "\n",
    "solved by\n",
    "\n",
    "$$V_{k+1}(x) = \\min_u(c(x, u) + \\lambda V_k(f(x, u)))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Proof of Convergence** $(\\lambda < 1)$:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "|BV_1(x) - BV_2(x)| &= |\\min_u(c(x, u) + \\lambda V_1(f(x, u))) - \\min_u(c(x, u) + \\lambda V_2(f(x, u)))| \\\\\n",
    "&\\leq \\max_u|\\lambda V_1(f(x,u))- \\lambda V_2(f(x,u))| \\\\\n",
    "&= \\lambda \\max_u| V_1(f(x,u))-  V_2(f(x,u))|\n",
    "\n",
    "\\end{align*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For policy $u=\\Pi(x)$ the value function is\n",
    "\n",
    "$$V^\\Pi(x) = \\min_u(c(x, \\Pi(x)) + \\lambda V^\\Pi(f(x, \\Pi(x))))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Initalise Policy $\\Pi$\n",
    "2. Compute $V^\\Pi$ til convergence\n",
    "3. Update $\\Pi$ to greedy w.r.t. $V^\\Pi$\n",
    "\n",
    "    $$\\Pi(x) = \\argmin_u(c(x, \\Pi(x)) + \\lambda V^\\Pi(f(x, \\Pi(x))))$$\n",
    "\n",
    "4. Repeat 2 and 3 til policy converges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem**:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "V^\\Pi(x) &\\geq \\min_u \\left( c(x, \\Pi'(x)) + \\lambda V^\\Pi(f(x, \\Pi '(x))) \\right) \\\\\n",
    "&= V^{\\Pi '}(x)\n",
    "\\end{align*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MDP Case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The theory above applied to a MDP with a stochastic policy yields\n",
    "\n",
    "$$V^\\Pi(s) = \\sum_a \\Pi(a|s) \\sum_{s' \\in S} \\sum_{r \\in R} p(s', r|s, a)(r + \\lambda V^\\Pi(s'))$$\n",
    "\n",
    "The optimal is hence\n",
    "\n",
    "$$V(s) = \\max_a \\sum_{s' \\in S} \\sum_{r \\in R} p(s', r|s, a)(r + \\lambda V(s'))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, learning the value function from samples in not usually done. Instead we use the action-value function defined as\n",
    "\n",
    "$$Q(s, a) = \\sum_{s' \\in S} \\sum_{r \\in R} p(s', r|s, a)(r + \\lambda V(s'))$$\n",
    "\n",
    "The link to the optimal value function above is clearly seen, with the optimal action similarly determined.\n",
    "\n",
    "The recursion property is also maintained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Q(s, a) = \\sum_{s' \\in S} \\sum_{r \\in R} p(s', r|s, a)(r + \\lambda \\max_b Q(s', b))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo Exploring Starts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple MC algorithm is possible with the value function, but is slow to converge as many samples are required for each policy, of which there may be many. **MCES** instead uses the Q function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat:\n",
    "\n",
    "- Choose random start $s_0$ and action $a_0$, then simulate trajectory  until $s_n \\in X_s$:\n",
    "    - $r_0, s_1 \\sim  p(s_1, r_0|s_0, a_0)$\n",
    "    - $a_1 = \\argmax_a Q(s_1, a)$\n",
    "\n",
    "\n",
    "- Let $R_n = r_n, \\quad R_k = r_k + Î»R_{k+1}, \\quad k = n-1,...,0$\n",
    "\n",
    "- Update Q values for k = 0,...,n:\n",
    "$$\\begin{align*} Q(s_k,a_k) \\leftarrow& \\frac{N(s_k,a_k)-1}{N(s_k,a_k)} Q(s_k,a_k) + \\frac{1}{N(s_k,a_k)} R_k \\\\\n",
    " \\leftarrow& Q(s_k,a_k) + \\frac{1}{N(s_k,a_k)}(R_k - Q(s_k,a_k))\\end{align*}$$\n",
    "\n",
    "\n",
    "N(s,a) is number of times state-action pair s,a. \n",
    "\n",
    "There are three approaches to deciding when to update the Q for a state-action pair:\n",
    "1. Update each time it is visited (Gives biased estimates because of revisits)\n",
    "2. Just update $s_0, a_0$ (Wastes lots of samples)\n",
    "3. Update each time a state-action pair is visited for the first time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the base MC method, **MCES** has not yet been proven to converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal Difference Mehtods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TD methods update after each sample unlike MC methods which update after a full trajectory has been sampled. This means that after each sample, the update ensures the next samples are more optimal as the information revealed by the previous step has already been implemented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an off-policy method as the values of the optimal policy can be obtained via any policy such as\n",
    "\n",
    "$$\\pi(s) = \\begin{cases}\n",
    "\\arg\\max_a Q(s,a), & \\text{with probability } 1-\\epsilon \\\\\n",
    "\\text{randomly selected}, & \\text{with probability } \\epsilon\n",
    "\\end{cases}$$\n",
    "\n",
    "After each sample we update the Q function using \n",
    "\n",
    "$$Q_{k+1}(s,a) = (1-\\alpha_k)Q_k(s,a) + \\alpha_k(r + \\lambda \\max_b Q_k(s',b))$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convergence is given by\n",
    "\n",
    "$$ \\{N(s,a) = \\infty, \\forall s \\in \\mathbb{S}, \\forall a\\in \\mathbb{A}\\},\\quad \\sum \\alpha_k = \\infty, \\quad \\sum \\alpha^2_k < 0 \\implies  Q_k \\rightarrow Q_{optimal}$$\n",
    "\n",
    "If deterministic $\\alpha_k = 1$ will converge to0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example is given in [`QLearning.py`](QLearning.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SARSA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SARSA is an on-policy method as it assumes finite actions are obtained with the current policy.\n",
    "\n",
    "$$Q_{k+1}^\\pi(s,a) = (1-\\alpha_k)Q_k^\\pi(s,a) + \\alpha_k(r + \\lambda Q_k^\\pi(s',\\pi(s')))$$\n",
    "\n",
    "Converges when $ \\{N(s,a) = \\infty, \\forall s \\in \\mathbb{S}, \\forall a\\in \\mathbb{A}\\}$ and the policies converge to the greeedy policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The update method given in [`QLearning.py`](QLearning.py) can be altered to use the policy instead of 'np.max(self.q_table[next_state])' to yield the SARSA method."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
