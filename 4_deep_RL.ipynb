{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q-learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more complex problems, a discrete state-action space may not be feasible. Instead we create a function approximation\n",
    "\n",
    "$$Q_\\theta(s,a)$$\n",
    "\n",
    "Where $\\theta$ are the function parameters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now seek to minimise the loss function\n",
    "\n",
    "$$ L(\\theta) = \\mathbb{E}(y_k - Q_\\theta(x_k, a_k))^2 $$\n",
    "\n",
    "where\n",
    "\n",
    "$$y_k = c_k + \\lambda \\min_a Q_\\theta(x_{k+1}, a)$$\n",
    "\n",
    "Therefore each time we perform our minimisation algorithm, we require a minibatch of quartets\n",
    "\n",
    "$$e_k = (x_k, a_k, c_k, x_{k+1})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like the Q-learning decscribed above, this is an off-policy method. Although this can be unstable, hence we may use\n",
    "\n",
    "$$y_k = c_k + \\lambda \\min_a Q_{\\theta'}(x_{k+1}, a)$$\n",
    "\n",
    "where $Q_{\\theta'}(x_{k}, a)$ tracks $Q_{\\theta}(x_{k}, a)$ by periodically using $\\theta' = \\theta $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "\n",
    "class Quartets(Dataset):\n",
    "    def __init__(self, csv_file: str, smiles_col=\"SMILES\", target_col=\"Solubility\"):\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx) -> Tuple[torch.FloatTensor, torch.FloatTensor, torch.FloatTensor, torch.FloatTensor]:\n",
    "        return \n",
    "\n",
    "\n",
    "class Qfunc(nn.Module):\n",
    "    def __init__(self, input_size: int = 2048):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, input_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(input_size // 2, input_size // 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(input_size // 4, input_size // 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(input_size // 8, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        criterion: nn.Module,\n",
    "        optimizer: optim.Optimizer,\n",
    "        device: torch.device,\n",
    "    ) -> None:\n",
    "        self.model = model.to(device)\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.device = device\n",
    "        self.train_losses: List[float] = []\n",
    "\n",
    "    def train_epoch(self, train_loader: DataLoader) -> float:\n",
    "        self.model.train()\n",
    "        total_loss: float = 0\n",
    "\n",
    "        for batch_idx, (features, targets) in enumerate(train_loader):\n",
    "            features, targets = features.to(self.device), targets.to(self.device)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(features).squeeze()\n",
    "            loss = self.criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        return total_loss / len(train_loader)\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_r2(pred: torch.Tensor, true: torch.Tensor) -> float:\n",
    "        ss_tot = torch.sum((true - true.mean()) ** 2)\n",
    "        ss_res = torch.sum((true - pred) ** 2)\n",
    "        r2 = 1 - (ss_res / ss_tot)\n",
    "        return r2.item()\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        train_loader: DataLoader,\n",
    "        epochs: int\n",
    "    ) -> None:\n",
    "        for epoch in tqdm.tqdm(range(epochs)):\n",
    "            train_loss = self.train_epoch(train_loader)\n",
    "\n",
    "            self.train_losses.append(train_loss)\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{epochs}:\")\n",
    "            print(f\"Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "    def plot_training_history(self) -> None:\n",
    "        plt.figure(figsize=(12, 4))\n",
    "\n",
    "        plt.plot(self.train_losses, label=\"Train Loss\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Set device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = ...\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    # Initialize model, criterion, optimizer\n",
    "    model = Qfunc()\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "    # Create trainer and train\n",
    "    trainer = Trainer(model, criterion, optimizer, device)\n",
    "    trainer.train(train_loader, epochs=100)\n",
    "\n",
    "    # Plot training history\n",
    "    trainer.plot_training_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQLearning:\n",
    "    def __init__(self, env, q_model: Qfunc):\n",
    "        self.env = env\n",
    "        self.epsilon_init = 0.5\n",
    "        self.epsilon_final = 0.0\n",
    "\n",
    "        self.q_model = q_model\n",
    "\n",
    "    def policy(self, state, step_total):\n",
    "        # Epsilon-greedy action selection\n",
    "        return\n",
    "\n",
    "\n",
    "    def train(self, episodes):\n",
    "        # add method to train NN using \n",
    "        return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actor-critic Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above method has a notable problem, how can we gurantee performance with the presence of $\\min_a$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where we now introduce another neural network $\\Pi_w$ that determines the suitable action given a state. $y_k$ is now given by \n",
    "\n",
    "$$y_k = c_k + \\lambda \\min_a Q_{\\theta}(x_{k+1}, \\Pi_w(x_{k+1}))$$\n",
    "\n",
    "and the function to minimise for the actor is given by \n",
    "\n",
    "$$L_\\Pi(w) = \\mathbb{E}(Q_{\\theta}(x, \\Pi_w(x_))) $$\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
